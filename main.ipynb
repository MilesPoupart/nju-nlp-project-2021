{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 银行产品评论观点提取\n",
    "**南京大学计算机科学与技术系研究生选修课程《自然语言处理》（2021秋）课程项目作业**\n",
    "\n",
    "【2022年1月16日重大发现】惊悉本项目作业就是照搬的2021年第九届CCF大数据与计算智能大赛的赛题“产品评论观点提取”([链接](https://www.datafountain.cn/competitions/529))，官方也已在AI Studio平台给出[基线版本](https://aistudio.baidu.com/aistudio/projectdetail/2417709)，可在大赛官网下载数据集，也可在AI Studio平台搜索“产品评论观点提取”获取更多项目源码参考。也已有其他同学在AI Studio[公开了源码](https://aistudio.baidu.com/aistudio/projectdetail/3210928)。心累...\n",
    "\n",
    "本项目可在[百度AI Studio平台](https://aistudio.baidu.com/aistudio/projectdetail/3281489)在线运行和调试（需要使用GPU环境）。项目源码已上传[Github](https://github.com/MilesPoupart/nju-nlp-project-2021)。\n",
    "\n",
    "项目数据集已包含在本项目`/data/data122751`路径中，具体数据集说明和下载见[数据集详情页面](https://aistudio.baidu.com/aistudio/datasetdetail/122751)。\n",
    "\n",
    "本项目的代码主要参考了[这一博文](https://blog.csdn.net/weixin_41611054/article/details/118487666)（[GitHub项目地址](https://github.com/zhenhao-huang/paddlehub_ernie_emotion_analysis)），特表示感谢！\n",
    "\n",
    "## 项目要求\n",
    "现有有关银行及银行相关产品的若干条中文评论，要求对这些评论进行观点提取。具体而言，分为以下两个子任务：\n",
    "\n",
    "### 实体识别\n",
    "要求识别出原始评论文本中的实体及类型，并按BIO格式进行标注。<br>需要进行识别的实体有：银行、产品、用户评论中的名词及形容词，具体的标注标签及说明如下表所示：\n",
    "\n",
    "|标签|说明|\n",
    "|:----:|:----:|\n",
    "|B-BANK|代表银行实体的开始|\n",
    "|I-BANK|代表银行实体的内部|\n",
    "|B-PRODUCT|代表产品实体的开始|\n",
    "|I-PRODUCT|代表产品实体的内部|\n",
    "|B-COMMENTS_N|代表用户评论（名词）|\n",
    "|I-COMMENTS_N|代表用户评论（名词）实体的内部|\n",
    "|B-COMMENTS_ADJ|代表用户评论（形容词）|\n",
    "|I-COMMENTS_ADJ|代表用户评论（形容词）实体的内部|\n",
    "|O|代表不属于标注的范围|\n",
    "\n",
    "下图是一个标注的例子：\n",
    "![实体识别的标注举例](https://ai-studio-static-online.cdn.bcebos.com/4966caab43894ab8ad1a7a85bdaff87a8437b4bf20b74ab0ad379c7cdcf1a248)\n",
    "\n",
    "### 情感分类\n",
    "根据用户评论的文本内容，判断其情感极性，并对其情感进行分类。<br>本次实验任务中，需要将用户的评论划分为正面（1）、负面（0）和中立（2）三种类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "本节代码主要对应to_tsv.py\n",
    "### 导入原始数据\n",
    "\n",
    "分别读入两个任务需要的数据，并将数据进行打乱。\n",
    "\n",
    "整个数据集共有6022条评论数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:43:23.360017Z",
     "iopub.status.busy": "2022-01-15T08:43:23.359129Z",
     "iopub.status.idle": "2022-01-15T08:43:23.817882Z",
     "shell.execute_reply": "2022-01-15T08:43:23.817091Z",
     "shell.execute_reply.started": "2022-01-15T08:43:23.359965Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6022\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"data/data122751/train.csv\"\n",
    "classify_rawtext = pd.read_csv(file_path, sep=\",\", usecols=['class', 'text'])\n",
    "classify_rawtext = classify_rawtext.sample(frac=1)  # 打乱数据集\n",
    "print(len(classify_rawtext))\n",
    "ner_text = pd.read_csv(file_path, sep=\",\", usecols=['text', 'BIO_anno'])\n",
    "ner_text = ner_text.sample(frac=1)  # 打乱数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理情感分类数据\n",
    "根据PaddleHub对[文本分类自定义数据的要求](https://paddlehub.readthedocs.io/zh_CN/release-v2.1/finetune/customized_dataset.html#id11)进行处理。\n",
    "+ text和class的列位置，使得class列在前，符合输入要求\n",
    "+ 按95%：5%划分训练集和验证集，充分利用训练数据。这里划分的测试集与验证集完全相同，仅用于程序计算相关指标。\n",
    "+ 整个数据集分类标签为0、1、2（情感分类为负面、正面、中立）的比例分别大致为10%、4%、86%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:43:35.772489Z",
     "iopub.status.busy": "2022-01-15T08:43:35.771317Z",
     "iopub.status.idle": "2022-01-15T08:43:35.818245Z",
     "shell.execute_reply": "2022-01-15T08:43:35.817547Z",
     "shell.execute_reply.started": "2022-01-15T08:43:35.772430Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'class']\n",
      "['class', 'text']\n",
      "classify_train_data \t {0: 0.106, 1: 0.038, 2: 0.856} 5720\n",
      "classify_dev_data \t {0: 0.093, 1: 0.043, 2: 0.864} 302\n",
      "classify_test_data \t {0: 0.093, 1: 0.043, 2: 0.864} 302\n"
     ]
    }
   ],
   "source": [
    "gen_classify = True  # 控制是否生成情感分类训练数据\n",
    "\n",
    "if gen_classify:\n",
    "    # 交换列位置\n",
    "    cols = list(classify_rawtext)\n",
    "    print(cols)\n",
    "    cols.insert(0, cols.pop(cols.index('class')))\n",
    "    print(cols)\n",
    "    classify_text = classify_rawtext.loc[:, cols]\n",
    "\n",
    "    # 划分训练集、验证集、测试集\n",
    "    classify_train = classify_text[:int(len(classify_text) * 0.95)]\n",
    "    # classify_text[int(len(classify_text) * 0.8):int(len(classify_text) * 0.9)]\n",
    "    classify_dev = classify_text[int(len(classify_text) * 0.95):]\n",
    "    classify_test = classify_text[int(len(classify_text) * 0.95):]\n",
    "    # 分别保存为tsv文件\n",
    "    classify_train.to_csv('data/data122751/classify_train_data.tsv',\n",
    "                          sep='\\t', header=None, index=False, columns=None, mode=\"w\")\n",
    "    classify_dev.to_csv('data/data122751/classify_dev_data.tsv',\n",
    "                        sep='\\t', header=None, index=False, columns=None, mode=\"w\")\n",
    "    classify_test.to_csv('data/data122751/classify_test_data.tsv',\n",
    "                         sep='\\t', header=None, index=False, columns=None, mode=\"w\")\n",
    "\n",
    "    # 验证训练集和测试集标签分布是否均匀\n",
    "    for file in ['classify_train_data', 'classify_dev_data', 'classify_test_data']:\n",
    "        file_path = f\"data/data122751/{file}.tsv\"\n",
    "        text = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "        prob = dict()\n",
    "        total = len(text[0])\n",
    "        for i in text[0]:\n",
    "            if prob.get(i) is None:\n",
    "                prob[i] = 1\n",
    "            else:\n",
    "                prob[i] += 1\n",
    "        # 按标签排序\n",
    "        prob = {i[0]: round(i[1] / total, 3)\n",
    "                for i in sorted(prob.items(), key=lambda k: k[0])}\n",
    "        print(file, '\\t', prob, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理实体标注数据\n",
    "根据PaddleHub对[序列标注自定义数据的要求](https://paddlehub.readthedocs.io/zh_CN/release-v2.1/finetune/customized_dataset.html#id16)进行处理。\n",
    "\n",
    "在实体标注的数据集中插入分隔符，方便模型进行后续的分字处理，并将label与token一一对应。例如以“/”作为分隔符，可以将text=“招行15.0W”，labels=“B-BANK I-BANK O O O O O”的一组数据处理成为text=“招/行/1/5/./0/W”，labels=“B-BANK/I-BANK/O/O/O/O/O”。\n",
    "\n",
    "通过检查数据集中没有出现过的特殊符号，可以发现仅有少数符号如\\#、￥、|、 \\等没有在数据集的文本中出现，不会引起错误的分割。\n",
    "\n",
    "但在后续的实验中会发现，上述的这些符号在tokenize阶段都会转换为[UNK]，且模型在最后预测时无法处理已插入分隔符的文本，会对分隔符也生成一个预测标签（大多数情况下是O），进而导致预测结果极度混乱。这说明如果无法对分隔符号进行tokenize，有可能在处理时无法识别分隔符并对其正确处理。(*若要验证这一观察，可将split_char进行修改，并在预测时输出部分结果进行观察*)\n",
    "\n",
    "在参考[相关官方文档中序列标注和命名实体识别的样例代码](https://paddlehub.readthedocs.io/zh_CN/release-v2.1/finetune/sequence_labeling.html)所采用的数据集[MSRA NER](https://bj.bcebos.com/paddlehub-dataset/msra_ner.tar.gz)后，可以发现这一数据集中的分隔符是'\\002'，经测试，这一分隔符可以使得相关接口调用后正常识别并分字。因此，最终选用'\\002'作为分隔符对序列标注的数据进行预处理。\n",
    "\n",
    "同样按95%：5%划分训练集和验证集，充分利用训练数据。这里划分的测试集与验证集完全相同，仅用于程序计算相关指标。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:43:40.777016Z",
     "iopub.status.busy": "2022-01-15T08:43:40.775747Z",
     "iopub.status.idle": "2022-01-15T08:43:42.715507Z",
     "shell.execute_reply": "2022-01-15T08:43:42.714780Z",
     "shell.execute_reply.started": "2022-01-15T08:43:40.776969Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\t交行14年用过，半年准备提额，却直接被降到1ｋ，半年期间只t过一次三千，其它全部真实消费，第六个月的时候为了增加评分提额，还特意分期两万，但降额后电话投诉，申请提...\n",
      "Split text:\t交\u0002行\u00021\u00024\u0002年\u0002用\u0002过\u0002，\u0002半\u0002年\u0002准\u0002备\u0002提\u0002额\u0002，\u0002却\u0002直\u0002接\u0002被\u0002降\u0002到\u00021\u0002ｋ\u0002，\u0002半\u0002年\u0002期\u0002间\u0002只\u0002t\u0002过\u0002一\u0002次\u0002三\u0002千\u0002，\u0002其\u0002它\u0002全\u0002部\u0002真\u0002实\u0002消\u0002费\u0002，\u0002第\u0002六\u0002个\u0002月\u0002的\u0002时\u0002候\u0002为\u0002了\u0002增\u0002加\u0002评\u0002分\u0002提\u0002额\u0002，\u0002还\u0002特\u0002意\u0002分\u0002期\u0002两\u0002万\u0002，\u0002但\u0002降\u0002额\u0002后\u0002电\u0002话\u0002投\u0002诉\u0002，\u0002申\u0002请\u0002提\u0002.\u0002.\u0002.\n",
      "Raw label:\tB-BANK I-BANK O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ O O O O O O O O O O O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N O O B-COMMENTS_N I-COMMENTS_N O O O O B-PRODUCT I-PRODUCT O O O O B-COMMENTS_ADJ O O O O O O O O O O O O O\n",
      "Split label:\tB-BANK\u0002I-BANK\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-COMMENTS_N\u0002I-COMMENTS_N\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-COMMENTS_ADJ\u0002I-COMMENTS_ADJ\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-COMMENTS_N\u0002I-COMMENTS_N\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-COMMENTS_N\u0002I-COMMENTS_N\u0002O\u0002O\u0002B-COMMENTS_N\u0002I-COMMENTS_N\u0002O\u0002O\u0002O\u0002O\u0002B-PRODUCT\u0002I-PRODUCT\u0002O\u0002O\u0002O\u0002O\u0002B-COMMENTS_ADJ\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\n",
      "\n",
      "Raw text:\t单标我有了，最近visa双标返现活动好\n",
      "Split text:\t单\u0002标\u0002我\u0002有\u0002了\u0002，\u0002最\u0002近\u0002v\u0002i\u0002s\u0002a\u0002双\u0002标\u0002返\u0002现\u0002活\u0002动\u0002好\n",
      "Raw label:\tB-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PRODUCT I-PRODUCT I-PRODUCT B-PRODUCT I-PRODUCT B-COMMENTS_N I-COMMENTS_N I-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ\n",
      "Split label:\tB-PRODUCT\u0002I-PRODUCT\u0002O\u0002O\u0002O\u0002O\u0002O\u0002O\u0002B-PRODUCT\u0002I-PRODUCT\u0002I-PRODUCT\u0002I-PRODUCT\u0002B-PRODUCT\u0002I-PRODUCT\u0002B-COMMENTS_N\u0002I-COMMENTS_N\u0002I-COMMENTS_N\u0002I-COMMENTS_N\u0002B-COMMENTS_ADJ\n",
      "\n",
      "Raw text:\t建设银行提额很慢的……\n",
      "Split text:\t建\u0002设\u0002银\u0002行\u0002提\u0002额\u0002很\u0002慢\u0002的\u0002…\u0002…\n",
      "Raw label:\tB-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COMMENTS_ADJ O O O\n",
      "Split label:\tB-BANK\u0002I-BANK\u0002I-BANK\u0002I-BANK\u0002B-COMMENTS_N\u0002I-COMMENTS_N\u0002B-COMMENTS_ADJ\u0002I-COMMENTS_ADJ\u0002O\u0002O\u0002O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_ner = True  # 控制是否生成实体识别数据\n",
    "split_char = \"\\002\"  # 分隔符\n",
    "if gen_ner:\n",
    "    sentence_cnt = len(ner_text)\n",
    "    for i in range(sentence_cnt):\n",
    "        raw_sentence = ner_text[\"text\"][i].lower()  # 全部转小写\n",
    "        if i <= 2:\n",
    "            print(\"Raw text:\\t%s\" % raw_sentence)\n",
    "        raw_sentence = split_char.join(raw_sentence)  # 文本逐字插入分隔符\n",
    "        if i <= 2:\n",
    "            print(\"Split text:\\t%s\" % raw_sentence)\n",
    "        ner_text.loc[i, \"text\"] = raw_sentence  # 保存结果\n",
    "\n",
    "        raw_lable = ner_text[\"BIO_anno\"][i]\n",
    "        if i <= 2:\n",
    "            print(\"Raw label:\\t%s\" % raw_lable)\n",
    "        raw_lable = raw_lable.replace(\" \", split_char)  # 标签间插入分隔符\n",
    "        if i <= 2:\n",
    "            print(\"Split label:\\t%s\" % raw_lable)\n",
    "            print(\"\")\n",
    "        ner_text.loc[i, \"BIO_anno\"] = raw_lable  # 保存结果\n",
    "\n",
    "    # 划分训练集验证集测试集\n",
    "    ner_train = ner_text[:int(len(ner_text) * 0.95)]\n",
    "    # ner_text[int(len(ner_text) * 0.8):int(len(ner_text) * 0.9)]\n",
    "    ner_dev = ner_text[int(len(ner_text) * 0.95):]\n",
    "    ner_test = ner_text[int(len(ner_text) * 0.95):]\n",
    "\n",
    "    # 保存到文件\n",
    "    ner_train.to_csv('data/data122751/ner_train_data.tsv',\n",
    "                     sep='\\t', header=None, index=False, columns=None, mode=\"w\")\n",
    "    ner_dev.to_csv('data/data122751/ner_dev_data.tsv', sep='\\t',\n",
    "                   header=None, index=False, columns=None, mode=\"w\")\n",
    "    ner_test.to_csv('data/data122751/ner_test_data.tsv', sep='\\t',\n",
    "                    header=None, index=False, columns=None, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理最终测试集\n",
    "将测试集也转换为tsv格式，情感分类的数据文件直接转换即可，实体识别的数据需要插入分隔符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:43:50.520331Z",
     "iopub.status.busy": "2022-01-15T08:43:50.519109Z",
     "iopub.status.idle": "2022-01-15T08:43:50.932950Z",
     "shell.execute_reply": "2022-01-15T08:43:50.932117Z",
     "shell.execute_reply.started": "2022-01-15T08:43:50.520277Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_final = True  # 控制是否生成最终测试集\n",
    "if gen_final:\n",
    "    # 生成情感分类任务的测试集\n",
    "    finaltest = pd.read_csv(\"data/data122751/test.csv\", sep=\",\")\n",
    "    finaltest.to_csv('data/data122751/classify_finaltest_data.tsv',\n",
    "                     sep='\\t', header=None, index=False, columns=None, mode=\"w\")\n",
    "    # 生成实体标注任务的测试集\n",
    "    for i in range(len(finaltest)):\n",
    "        raw_sentence = finaltest[\"text\"][i]\n",
    "        raw_sentence = split_char.join(raw_sentence)  # 插入分隔符\n",
    "        finaltest.loc[i, \"text\"] = raw_sentence\n",
    "    finaltest.to_csv('data/data122751/ner_finaltest_data.tsv',\n",
    "                     sep='\\t', header=None, index=False, columns=None, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感分类\n",
    "### 导入数据集\n",
    "根据[PaddleHub文档的要求](https://paddlehub.readthedocs.io/zh_CN/release-v2.1/finetune/customized_dataset.html#id15),需要继承基类TextClassificationDataset。按要求修改即可。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T09:02:51.733342Z",
     "iopub.status.busy": "2022-01-15T09:02:51.732280Z",
     "iopub.status.idle": "2022-01-15T09:02:51.739751Z",
     "shell.execute_reply": "2022-01-15T09:02:51.739098Z",
     "shell.execute_reply.started": "2022-01-15T09:02:51.733289Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddlehub as hub\n",
    "import ast\n",
    "import argparse\n",
    "from paddlehub.datasets.base_nlp_dataset import TextClassificationDataset\n",
    "\n",
    "\n",
    "class MyDataset(TextClassificationDataset):\n",
    "    # 数据集存放目录\n",
    "    base_path = 'data/data122751'\n",
    "    # 数据集的标签列表，多分类标签格式为['0', '1', '2', '3',...]\n",
    "    label_list = ['0', '1', '2']\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_len: int = 128, mode: str = 'train'):\n",
    "        if mode == 'train':\n",
    "            data_file = 'classify_train_data.tsv'\n",
    "        elif mode == 'test':\n",
    "            data_file = 'classify_test_data.tsv'\n",
    "        else:\n",
    "            data_file = 'classify_dev_data.tsv'\n",
    "        super().__init__(\n",
    "            base_path=self.base_path,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "            mode=mode,\n",
    "            data_file=data_file,\n",
    "            label_list=self.label_list,\n",
    "            is_file_with_header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置参数并训练\n",
    "使用Adam优化器，相关默认设置见[文档说明](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Adam_cn.html)。\n",
    "\n",
    "如要重新训练，请务必**指定一空目录保存参数**，或**删除默认的`ernie_checkpoint_classify`目录**，否则程序会用上一个epoch的参数继续训练！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T09:02:56.610133Z",
     "iopub.status.busy": "2022-01-15T09:02:56.609130Z",
     "iopub.status.idle": "2022-01-15T09:05:50.079883Z",
     "shell.execute_reply": "2022-01-15T09:05:50.079129Z",
     "shell.execute_reply.started": "2022-01-15T09:02:56.610093Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-15 17:02:56,617] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2022-01-15 17:02:58,411] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2022-01-15 17:02:59,651] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2022-01-15 17:02:59,728] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2022-01-15 17:02:59,808] [ WARNING] - PaddleHub model checkpoint not found, start from scratch...\n",
      "[2022-01-15 17:03:02,012] [   TRAIN] - Epoch=1/3, Step=10/179 loss=0.8716 acc=0.6250 lr=0.000050 step/sec=4.54 | ETA 00:01:58\n",
      "[2022-01-15 17:03:04,166] [   TRAIN] - Epoch=1/3, Step=20/179 loss=0.4965 acc=0.8594 lr=0.000050 step/sec=4.64 | ETA 00:01:56\n",
      "[2022-01-15 17:03:06,318] [   TRAIN] - Epoch=1/3, Step=30/179 loss=0.5548 acc=0.8187 lr=0.000050 step/sec=4.65 | ETA 00:01:56\n",
      "[2022-01-15 17:03:08,468] [   TRAIN] - Epoch=1/3, Step=40/179 loss=0.4147 acc=0.8781 lr=0.000050 step/sec=4.65 | ETA 00:01:56\n",
      "[2022-01-15 17:03:10,617] [   TRAIN] - Epoch=1/3, Step=50/179 loss=0.4661 acc=0.8469 lr=0.000050 step/sec=4.65 | ETA 00:01:56\n",
      "[2022-01-15 17:03:12,766] [   TRAIN] - Epoch=1/3, Step=60/179 loss=0.3961 acc=0.8625 lr=0.000050 step/sec=4.65 | ETA 00:01:55\n",
      "[2022-01-15 17:03:14,916] [   TRAIN] - Epoch=1/3, Step=70/179 loss=0.4645 acc=0.8656 lr=0.000050 step/sec=4.65 | ETA 00:01:55\n",
      "[2022-01-15 17:03:17,070] [   TRAIN] - Epoch=1/3, Step=80/179 loss=0.4310 acc=0.8469 lr=0.000050 step/sec=4.64 | ETA 00:01:55\n",
      "[2022-01-15 17:03:19,216] [   TRAIN] - Epoch=1/3, Step=90/179 loss=0.3648 acc=0.8781 lr=0.000050 step/sec=4.66 | ETA 00:01:55\n",
      "[2022-01-15 17:03:21,366] [   TRAIN] - Epoch=1/3, Step=100/179 loss=0.3172 acc=0.8906 lr=0.000050 step/sec=4.65 | ETA 00:01:55\n",
      "[2022-01-15 17:03:23,520] [   TRAIN] - Epoch=1/3, Step=110/179 loss=0.3127 acc=0.8812 lr=0.000050 step/sec=4.64 | ETA 00:01:55\n",
      "[2022-01-15 17:03:25,682] [   TRAIN] - Epoch=1/3, Step=120/179 loss=0.3651 acc=0.8688 lr=0.000050 step/sec=4.62 | ETA 00:01:55\n",
      "[2022-01-15 17:03:27,840] [   TRAIN] - Epoch=1/3, Step=130/179 loss=0.3029 acc=0.9000 lr=0.000050 step/sec=4.64 | ETA 00:01:55\n",
      "[2022-01-15 17:03:30,008] [   TRAIN] - Epoch=1/3, Step=140/179 loss=0.3320 acc=0.8656 lr=0.000050 step/sec=4.61 | ETA 00:01:55\n",
      "[2022-01-15 17:03:32,164] [   TRAIN] - Epoch=1/3, Step=150/179 loss=0.3443 acc=0.8844 lr=0.000050 step/sec=4.64 | ETA 00:01:55\n",
      "[2022-01-15 17:03:34,321] [   TRAIN] - Epoch=1/3, Step=160/179 loss=0.2990 acc=0.8938 lr=0.000050 step/sec=4.64 | ETA 00:01:55\n",
      "[2022-01-15 17:03:36,480] [   TRAIN] - Epoch=1/3, Step=170/179 loss=0.2161 acc=0.9156 lr=0.000050 step/sec=4.63 | ETA 00:01:55\n",
      "[2022-01-15 17:03:39,148] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_acc=0.8808\n",
      "[2022-01-15 17:03:51,767] [    EVAL] - Saving best model to ./ernie_checkpoint_classify/best_model [best acc=0.8808]\n",
      "[2022-01-15 17:03:51,771] [    INFO] - Saving model checkpoint to ./ernie_checkpoint_classify/epoch_1\n",
      "[2022-01-15 17:04:06,657] [   TRAIN] - Epoch=2/3, Step=10/179 loss=0.2085 acc=0.9187 lr=0.000050 step/sec=0.63 | ETA 00:03:09\n",
      "[2022-01-15 17:04:08,814] [   TRAIN] - Epoch=2/3, Step=20/179 loss=0.2515 acc=0.9156 lr=0.000050 step/sec=4.64 | ETA 00:03:06\n",
      "[2022-01-15 17:04:10,995] [   TRAIN] - Epoch=2/3, Step=30/179 loss=0.2264 acc=0.9219 lr=0.000050 step/sec=4.59 | ETA 00:03:02\n",
      "[2022-01-15 17:04:13,153] [   TRAIN] - Epoch=2/3, Step=40/179 loss=0.2152 acc=0.9187 lr=0.000050 step/sec=4.63 | ETA 00:02:59\n",
      "[2022-01-15 17:04:15,309] [   TRAIN] - Epoch=2/3, Step=50/179 loss=0.2272 acc=0.9125 lr=0.000050 step/sec=4.64 | ETA 00:02:57\n",
      "[2022-01-15 17:04:17,470] [   TRAIN] - Epoch=2/3, Step=60/179 loss=0.2132 acc=0.9281 lr=0.000050 step/sec=4.63 | ETA 00:02:54\n",
      "[2022-01-15 17:04:19,627] [   TRAIN] - Epoch=2/3, Step=70/179 loss=0.1947 acc=0.9281 lr=0.000050 step/sec=4.64 | ETA 00:02:52\n",
      "[2022-01-15 17:04:21,785] [   TRAIN] - Epoch=2/3, Step=80/179 loss=0.2389 acc=0.9125 lr=0.000050 step/sec=4.63 | ETA 00:02:49\n",
      "[2022-01-15 17:04:23,945] [   TRAIN] - Epoch=2/3, Step=90/179 loss=0.1745 acc=0.9406 lr=0.000050 step/sec=4.63 | ETA 00:02:47\n",
      "[2022-01-15 17:04:26,103] [   TRAIN] - Epoch=2/3, Step=100/179 loss=0.1982 acc=0.9344 lr=0.000050 step/sec=4.63 | ETA 00:02:46\n",
      "[2022-01-15 17:04:28,264] [   TRAIN] - Epoch=2/3, Step=110/179 loss=0.2749 acc=0.9000 lr=0.000050 step/sec=4.63 | ETA 00:02:44\n",
      "[2022-01-15 17:04:30,435] [   TRAIN] - Epoch=2/3, Step=120/179 loss=0.2294 acc=0.9125 lr=0.000050 step/sec=4.61 | ETA 00:02:42\n",
      "[2022-01-15 17:04:32,601] [   TRAIN] - Epoch=2/3, Step=130/179 loss=0.2584 acc=0.8875 lr=0.000050 step/sec=4.62 | ETA 00:02:41\n",
      "[2022-01-15 17:04:34,757] [   TRAIN] - Epoch=2/3, Step=140/179 loss=0.2008 acc=0.9125 lr=0.000050 step/sec=4.64 | ETA 00:02:39\n",
      "[2022-01-15 17:04:36,924] [   TRAIN] - Epoch=2/3, Step=150/179 loss=0.2431 acc=0.9156 lr=0.000050 step/sec=4.61 | ETA 00:02:38\n",
      "[2022-01-15 17:04:39,085] [   TRAIN] - Epoch=2/3, Step=160/179 loss=0.1721 acc=0.9313 lr=0.000050 step/sec=4.63 | ETA 00:02:37\n",
      "[2022-01-15 17:04:41,249] [   TRAIN] - Epoch=2/3, Step=170/179 loss=0.1862 acc=0.9344 lr=0.000050 step/sec=4.62 | ETA 00:02:36\n",
      "[2022-01-15 17:04:43,953] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_acc=0.8775\n",
      "[2022-01-15 17:04:43,956] [    INFO] - Saving model checkpoint to ./ernie_checkpoint_classify/epoch_2\n",
      "[2022-01-15 17:04:59,000] [   TRAIN] - Epoch=3/3, Step=10/179 loss=0.1295 acc=0.9563 lr=0.000050 step/sec=1.07 | ETA 00:02:53\n",
      "[2022-01-15 17:05:01,162] [   TRAIN] - Epoch=3/3, Step=20/179 loss=0.1094 acc=0.9625 lr=0.000050 step/sec=4.63 | ETA 00:02:52\n",
      "[2022-01-15 17:05:03,329] [   TRAIN] - Epoch=3/3, Step=30/179 loss=0.1300 acc=0.9500 lr=0.000050 step/sec=4.61 | ETA 00:02:50\n",
      "[2022-01-15 17:05:05,489] [   TRAIN] - Epoch=3/3, Step=40/179 loss=0.0941 acc=0.9656 lr=0.000050 step/sec=4.63 | ETA 00:02:49\n",
      "[2022-01-15 17:05:07,647] [   TRAIN] - Epoch=3/3, Step=50/179 loss=0.0740 acc=0.9719 lr=0.000050 step/sec=4.63 | ETA 00:02:48\n",
      "[2022-01-15 17:05:09,819] [   TRAIN] - Epoch=3/3, Step=60/179 loss=0.1551 acc=0.9437 lr=0.000050 step/sec=4.60 | ETA 00:02:47\n",
      "[2022-01-15 17:05:11,981] [   TRAIN] - Epoch=3/3, Step=70/179 loss=0.0946 acc=0.9656 lr=0.000050 step/sec=4.63 | ETA 00:02:45\n",
      "[2022-01-15 17:05:14,147] [   TRAIN] - Epoch=3/3, Step=80/179 loss=0.1380 acc=0.9500 lr=0.000050 step/sec=4.62 | ETA 00:02:44\n",
      "[2022-01-15 17:05:16,310] [   TRAIN] - Epoch=3/3, Step=90/179 loss=0.1763 acc=0.9406 lr=0.000050 step/sec=4.62 | ETA 00:02:43\n",
      "[2022-01-15 17:05:18,478] [   TRAIN] - Epoch=3/3, Step=100/179 loss=0.1477 acc=0.9500 lr=0.000050 step/sec=4.61 | ETA 00:02:42\n",
      "[2022-01-15 17:05:20,643] [   TRAIN] - Epoch=3/3, Step=110/179 loss=0.1309 acc=0.9594 lr=0.000050 step/sec=4.62 | ETA 00:02:41\n",
      "[2022-01-15 17:05:22,808] [   TRAIN] - Epoch=3/3, Step=120/179 loss=0.1190 acc=0.9469 lr=0.000050 step/sec=4.62 | ETA 00:02:40\n",
      "[2022-01-15 17:05:24,978] [   TRAIN] - Epoch=3/3, Step=130/179 loss=0.1159 acc=0.9531 lr=0.000050 step/sec=4.61 | ETA 00:02:39\n",
      "[2022-01-15 17:05:27,142] [   TRAIN] - Epoch=3/3, Step=140/179 loss=0.1254 acc=0.9469 lr=0.000050 step/sec=4.62 | ETA 00:02:38\n",
      "[2022-01-15 17:05:29,304] [   TRAIN] - Epoch=3/3, Step=150/179 loss=0.1380 acc=0.9469 lr=0.000050 step/sec=4.63 | ETA 00:02:38\n",
      "[2022-01-15 17:05:31,474] [   TRAIN] - Epoch=3/3, Step=160/179 loss=0.1286 acc=0.9594 lr=0.000050 step/sec=4.61 | ETA 00:02:37\n",
      "[2022-01-15 17:05:33,642] [   TRAIN] - Epoch=3/3, Step=170/179 loss=0.1285 acc=0.9500 lr=0.000050 step/sec=4.61 | ETA 00:02:36\n",
      "[2022-01-15 17:05:36,323] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_acc=0.8775\n",
      "[2022-01-15 17:05:36,331] [    INFO] - Saving model checkpoint to ./ernie_checkpoint_classify/epoch_3\n",
      "[2022-01-15 17:05:50,072] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_acc=0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': defaultdict(int, {'acc': 0.8774834437086093})}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数设置\n",
    "num_epoch = 3  # Number of epoches for fine-tuning.\n",
    "use_gpu = True  # Whether use GPU for fine-tuning, input should be True or False\n",
    "learning_rate = 5e-5  # Learning rate used to train with warmup.\n",
    "max_seq_len = 128  # Number of words of the longest seqence.\n",
    "batch_size = 32  # Total examples' number in batch for training.\n",
    "checkpoint_dir = './ernie_checkpoint_classify'  # Directory to model checkpoint\n",
    "save_interval = 1  # Save checkpoint every n epoch.\n",
    "\n",
    "# 选择模型、任务和类别数\n",
    "model = hub.Module(name='ernie', task='seq-cls',\n",
    "                   num_classes=len(MyDataset.label_list))\n",
    "\n",
    "train_dataset = MyDataset(tokenizer=model.get_tokenizer(),\n",
    "                          max_seq_len=max_seq_len, mode='train')\n",
    "dev_dataset = MyDataset(tokenizer=model.get_tokenizer(),\n",
    "                        max_seq_len=max_seq_len, mode='dev')\n",
    "test_dataset = MyDataset(tokenizer=model.get_tokenizer(),\n",
    "                         max_seq_len=max_seq_len, mode='test')\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(\n",
    "    learning_rate=learning_rate, parameters=model.parameters())\n",
    "trainer = hub.Trainer(\n",
    "    model, optimizer, checkpoint_dir=checkpoint_dir, use_gpu=use_gpu)\n",
    "trainer.train(train_dataset, epochs=num_epoch, batch_size=batch_size,\n",
    "              eval_dataset=dev_dataset, save_interval=save_interval)\n",
    "# 在测试集上评估当前训练模型\n",
    "trainer.evaluate(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码块如有运行问题，可直接运行funetune_ernie-classify.py源文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果预测\n",
    "如要对验证集的进行预测并对其指标进行计算，可将final_predict改为`False`。\n",
    "\n",
    "我在本任务的训练中得到的最好的模型参数保存在`/ernie_checkpoint_classify_9505_2_acc93.71/best_model/model.pdparams`（由于AI Studio平台限制，公开文件大小不超过1G，参数文件可能会在其他平台公开）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T09:07:50.452410Z",
     "iopub.status.busy": "2022-01-15T09:07:50.451276Z",
     "iopub.status.idle": "2022-01-15T09:08:49.567476Z",
     "shell.execute_reply": "2022-01-15T09:08:49.566667Z",
     "shell.execute_reply.started": "2022-01-15T09:07:50.452366Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-15 17:07:50,465] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2022-01-15 17:07:55,722] [    INFO] - Loaded parameters from /home/aistudio/ernie_checkpoint_classify/best_model/model.pdparams\n",
      "[2022-01-15 17:07:55,843] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "final_predict = True\n",
    "if final_predict:\n",
    "    file_path = \"data/data122751/classify_finaltest_data.tsv\"\n",
    "else:\n",
    "    file_path = \"data/data122751/classify_test_data.tsv\"\n",
    "\n",
    "text = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "label_map = {0: 0, 1: 1, 2: 2}  # {0: 'negative', 1: 'positive', 2:'medium'}\n",
    "data = [[i] for i in text[1]]\n",
    "\n",
    "model = hub.Module(name='ernie', task='seq-cls',\n",
    "                   load_checkpoint='./ernie_checkpoint_classify/best_model/model.pdparams', label_map=label_map)\n",
    "results = model.predict(data, max_seq_len=128, batch_size=1, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出部分结果并保存结果/计算验证集指标。以下代码块仅实现了对准确率的计算，未计算项目评测使用的$\\kappa$指标，如有需要，可自行添加。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T09:09:08.465589Z",
     "iopub.status.busy": "2022-01-15T09:09:08.465058Z",
     "iopub.status.idle": "2022-01-15T09:09:08.496210Z",
     "shell.execute_reply": "2022-01-15T09:09:08.495494Z",
     "shell.execute_reply.started": "2022-01-15T09:09:08.465549Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['不能??好像房贷跟信用卡是分开审核的反正我的不得']\n",
      "2 ['我感觉这样才是合理的，花呗白条没要那么多信息，照样可以给额度。有征信威慑，没那么多人敢借了不还。与其眉毛胡子一把抓，还不如按额度区间对客户进行不同程度的调查，免...']\n",
      "0 ['这几天也接到了建行的分期电话，让我1.6分12期，跟他说5000，3期，意思意思。快两年了还没首提，心塞']\n",
      "2 ['我是建行贷款，别的银行信用卡没还，账单大概4w']\n",
      "0 ['银行耍无赖啊']\n",
      "2 ['晒晒“断”卡，我的招行卡猫咪卡']\n",
      "2 ['交行_197k\\u3000民生_217k\\u3000中行_125k\\u3000工行_158k']\n",
      "2 ['工商84000']\n",
      "2 ['201305招商银行201501建设银行']\n",
      "2 ['中信34K（淘宝V、i白、lu.xu.ry）']\n"
     ]
    }
   ],
   "source": [
    "# print(results)\n",
    "for i in range(10):\n",
    "    print(results[i], data[i])\n",
    "if final_predict:\n",
    "    resultsDF = pd.DataFrame(data=results)\n",
    "    resultsDF.to_csv('classify_result.csv', sep='\\t',\n",
    "                     header=None, index=False, columns=None, mode=\"w\")\n",
    "else:\n",
    "    # 输出测试集准确率\n",
    "    count = 0\n",
    "    for i, j in zip(text[0], results):\n",
    "        # print(type(i), type(j))\n",
    "        if int(i) == int(j):\n",
    "            count += 1\n",
    "    print(\"测试集准确率:\", count / len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实体识别\n",
    "### 导入数据集\n",
    "根据[PaddleHub文档的要求](https://paddlehub.readthedocs.io/zh_CN/release-v2.1/finetune/customized_dataset.html#id20),需要继承基类SeqLabelingDataset。按要求修改即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:55:40.873888Z",
     "iopub.status.busy": "2022-01-15T08:55:40.872675Z",
     "iopub.status.idle": "2022-01-15T08:55:40.880775Z",
     "shell.execute_reply": "2022-01-15T08:55:40.880113Z",
     "shell.execute_reply.started": "2022-01-15T08:55:40.873834Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddlehub.datasets.base_nlp_dataset import SeqLabelingDataset\n",
    "\n",
    "\n",
    "class MyDataset(SeqLabelingDataset):\n",
    "    # 数据集存放目录\n",
    "    base_path = 'data/data122751'\n",
    "    # 数据集的标签列表\n",
    "    label_list = ['B-BANK', 'I-BANK', 'B-PRODUCT', 'I-PRODUCT',\n",
    "                  'B-COMMENTS_N', 'I-COMMENTS_N', 'B-COMMENTS_ADJ', 'I-COMMENTS_ADJ', 'O']\n",
    "    label_map = {idx: label for idx, label in enumerate(label_list)}\n",
    "    # 数据文件使用的分隔符\n",
    "    split_char = '\\002'\n",
    "\n",
    "    def __init__(self, tokenizer, max_seq_len: int = 128, mode: str = 'train'):\n",
    "        if mode == 'train':\n",
    "            data_file = 'ner_train_data.tsv'\n",
    "        elif mode == 'test':\n",
    "            data_file = 'ner_test_data.tsv'\n",
    "        else:\n",
    "            data_file = 'ner_dev_data.tsv'\n",
    "        super().__init__(\n",
    "            base_path=self.base_path,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=max_seq_len,\n",
    "            mode=mode,\n",
    "            data_file=data_file,\n",
    "            label_file=None,\n",
    "            label_list=self.label_list,\n",
    "            split_char=self.split_char,\n",
    "            is_file_with_header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置参数并训练\n",
    "使用Adam优化器，相关默认设置见[文档说明](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/optimizer/Adam_cn.html)。\n",
    "\n",
    "如要重新训练，请务必**指定一空目录保存参数**，或**删除默认的`ernie_checkpoint_ner`目录**，否则程序会用上一个epoch的参数继续训练！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:55:48.905505Z",
     "iopub.status.busy": "2022-01-15T08:55:48.904293Z",
     "iopub.status.idle": "2022-01-15T08:59:31.835104Z",
     "shell.execute_reply": "2022-01-15T08:59:31.834232Z",
     "shell.execute_reply.started": "2022-01-15T08:55:48.905464Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-15 16:55:48,912] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2022-01-15 16:55:50,704] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2022-01-15 16:55:53,144] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2022-01-15 16:55:53,291] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n",
      "[2022-01-15 16:55:53,432] [ WARNING] - PaddleHub model checkpoint not found, start from scratch...\n",
      "[2022-01-15 16:55:53,621] [ WARNING] - Compatibility Warning: The params of ChunkEvaluator.compute has been modified. The old version is `inputs`, `lengths`, `predictions`, `labels` while the current version is `lengths`, `predictions`, `labels`.  Please update the usage.\n",
      "[2022-01-15 16:55:55,848] [   TRAIN] - Epoch=1/3, Step=10/179 loss=0.2755 f1_score=0.0050 lr=0.000050 step/sec=4.14 | ETA 00:02:09\n",
      "[2022-01-15 16:55:58,074] [   TRAIN] - Epoch=1/3, Step=20/179 loss=0.1296 f1_score=0.0000 lr=0.000050 step/sec=4.49 | ETA 00:02:04\n",
      "[2022-01-15 16:56:00,303] [   TRAIN] - Epoch=1/3, Step=30/179 loss=0.1110 f1_score=0.0732 lr=0.000050 step/sec=4.49 | ETA 00:02:02\n",
      "[2022-01-15 16:56:02,529] [   TRAIN] - Epoch=1/3, Step=40/179 loss=0.0842 f1_score=0.3417 lr=0.000050 step/sec=4.49 | ETA 00:02:02\n",
      "[2022-01-15 16:56:04,763] [   TRAIN] - Epoch=1/3, Step=50/179 loss=0.0720 f1_score=0.4662 lr=0.000050 step/sec=4.48 | ETA 00:02:01\n",
      "[2022-01-15 16:56:06,993] [   TRAIN] - Epoch=1/3, Step=60/179 loss=0.0504 f1_score=0.5315 lr=0.000050 step/sec=4.48 | ETA 00:02:01\n",
      "[2022-01-15 16:56:09,233] [   TRAIN] - Epoch=1/3, Step=70/179 loss=0.0510 f1_score=0.5865 lr=0.000050 step/sec=4.46 | ETA 00:02:01\n",
      "[2022-01-15 16:56:11,476] [   TRAIN] - Epoch=1/3, Step=80/179 loss=0.0503 f1_score=0.6143 lr=0.000050 step/sec=4.46 | ETA 00:02:01\n",
      "[2022-01-15 16:56:13,727] [   TRAIN] - Epoch=1/3, Step=90/179 loss=0.0461 f1_score=0.6379 lr=0.000050 step/sec=4.44 | ETA 00:02:01\n",
      "[2022-01-15 16:56:15,968] [   TRAIN] - Epoch=1/3, Step=100/179 loss=0.0399 f1_score=0.7121 lr=0.000050 step/sec=4.46 | ETA 00:02:01\n",
      "[2022-01-15 16:56:18,199] [   TRAIN] - Epoch=1/3, Step=110/179 loss=0.0460 f1_score=0.6348 lr=0.000050 step/sec=4.48 | ETA 00:02:00\n",
      "[2022-01-15 16:56:20,431] [   TRAIN] - Epoch=1/3, Step=120/179 loss=0.0392 f1_score=0.6686 lr=0.000050 step/sec=4.48 | ETA 00:02:00\n",
      "[2022-01-15 16:56:22,673] [   TRAIN] - Epoch=1/3, Step=130/179 loss=0.0416 f1_score=0.6694 lr=0.000050 step/sec=4.46 | ETA 00:02:00\n",
      "[2022-01-15 16:56:24,911] [   TRAIN] - Epoch=1/3, Step=140/179 loss=0.0384 f1_score=0.6666 lr=0.000050 step/sec=4.47 | ETA 00:02:00\n",
      "[2022-01-15 16:56:27,143] [   TRAIN] - Epoch=1/3, Step=150/179 loss=0.0363 f1_score=0.7212 lr=0.000050 step/sec=4.48 | ETA 00:02:00\n",
      "[2022-01-15 16:56:29,374] [   TRAIN] - Epoch=1/3, Step=160/179 loss=0.0358 f1_score=0.7219 lr=0.000050 step/sec=4.48 | ETA 00:02:00\n",
      "[2022-01-15 16:56:31,611] [   TRAIN] - Epoch=1/3, Step=170/179 loss=0.0368 f1_score=0.7255 lr=0.000050 step/sec=4.47 | ETA 00:02:00\n",
      "[2022-01-15 16:56:34,459] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - [Evaluation result] avg_f1_score=0.7495\n",
      "[2022-01-15 16:56:46,575] [    EVAL] - Saving best model to ./ernie_checkpoint_ner/best_model [best f1_score=0.7495]\n",
      "[2022-01-15 16:56:46,583] [    INFO] - Saving model checkpoint to ./ernie_checkpoint_ner/epoch_1\n",
      "[2022-01-15 16:57:01,660] [   TRAIN] - Epoch=2/3, Step=10/179 loss=0.0305 f1_score=0.7539 lr=0.000050 step/sec=0.63 | ETA 00:03:13\n",
      "[2022-01-15 16:57:03,889] [   TRAIN] - Epoch=2/3, Step=20/179 loss=0.0328 f1_score=0.7297 lr=0.000050 step/sec=4.48 | ETA 00:03:10\n",
      "[2022-01-15 16:57:06,124] [   TRAIN] - Epoch=2/3, Step=30/179 loss=0.0305 f1_score=0.7587 lr=0.000050 step/sec=4.48 | ETA 00:03:06\n",
      "[2022-01-15 16:57:08,359] [   TRAIN] - Epoch=2/3, Step=40/179 loss=0.0259 f1_score=0.7752 lr=0.000050 step/sec=4.48 | ETA 00:03:03\n",
      "[2022-01-15 16:57:10,598] [   TRAIN] - Epoch=2/3, Step=50/179 loss=0.0294 f1_score=0.7383 lr=0.000050 step/sec=4.47 | ETA 00:03:00\n",
      "[2022-01-15 16:57:12,844] [   TRAIN] - Epoch=2/3, Step=60/179 loss=0.0284 f1_score=0.7664 lr=0.000050 step/sec=4.45 | ETA 00:02:58\n",
      "[2022-01-15 16:57:15,088] [   TRAIN] - Epoch=2/3, Step=70/179 loss=0.0269 f1_score=0.7721 lr=0.000050 step/sec=4.46 | ETA 00:02:56\n",
      "[2022-01-15 16:57:17,348] [   TRAIN] - Epoch=2/3, Step=80/179 loss=0.0271 f1_score=0.7710 lr=0.000050 step/sec=4.42 | ETA 00:02:53\n",
      "[2022-01-15 16:57:19,595] [   TRAIN] - Epoch=2/3, Step=90/179 loss=0.0292 f1_score=0.7458 lr=0.000050 step/sec=4.45 | ETA 00:02:52\n",
      "[2022-01-15 16:57:21,837] [   TRAIN] - Epoch=2/3, Step=100/179 loss=0.0276 f1_score=0.7617 lr=0.000050 step/sec=4.46 | ETA 00:02:50\n",
      "[2022-01-15 16:57:24,076] [   TRAIN] - Epoch=2/3, Step=110/179 loss=0.0309 f1_score=0.7690 lr=0.000050 step/sec=4.47 | ETA 00:02:48\n",
      "[2022-01-15 16:57:26,322] [   TRAIN] - Epoch=2/3, Step=120/179 loss=0.0306 f1_score=0.7897 lr=0.000050 step/sec=4.45 | ETA 00:02:46\n",
      "[2022-01-15 16:57:28,567] [   TRAIN] - Epoch=2/3, Step=130/179 loss=0.0297 f1_score=0.7714 lr=0.000050 step/sec=4.45 | ETA 00:02:45\n",
      "[2022-01-15 16:57:30,803] [   TRAIN] - Epoch=2/3, Step=140/179 loss=0.0277 f1_score=0.7511 lr=0.000050 step/sec=4.47 | ETA 00:02:43\n",
      "[2022-01-15 16:57:33,042] [   TRAIN] - Epoch=2/3, Step=150/179 loss=0.0277 f1_score=0.7843 lr=0.000050 step/sec=4.47 | ETA 00:02:42\n",
      "[2022-01-15 16:57:35,281] [   TRAIN] - Epoch=2/3, Step=160/179 loss=0.0246 f1_score=0.7982 lr=0.000050 step/sec=4.47 | ETA 00:02:41\n",
      "[2022-01-15 16:57:37,527] [   TRAIN] - Epoch=2/3, Step=170/179 loss=0.0239 f1_score=0.7890 lr=0.000050 step/sec=4.45 | ETA 00:02:40\n",
      "[2022-01-15 16:57:40,344] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_f1_score=0.7974\n",
      "[2022-01-15 16:58:02,141] [    EVAL] - Saving best model to ./ernie_checkpoint_ner/best_model [best f1_score=0.7974]\n",
      "[2022-01-15 16:58:02,144] [    INFO] - Saving model checkpoint to ./ernie_checkpoint_ner/epoch_2\n",
      "[2022-01-15 16:58:16,987] [   TRAIN] - Epoch=3/3, Step=10/179 loss=0.0222 f1_score=0.8258 lr=0.000050 step/sec=0.48 | ETA 00:03:29\n",
      "[2022-01-15 16:58:19,225] [   TRAIN] - Epoch=3/3, Step=20/179 loss=0.0236 f1_score=0.8123 lr=0.000050 step/sec=4.47 | ETA 00:03:27\n",
      "[2022-01-15 16:58:21,459] [   TRAIN] - Epoch=3/3, Step=30/179 loss=0.0213 f1_score=0.8044 lr=0.000050 step/sec=4.48 | ETA 00:03:24\n",
      "[2022-01-15 16:58:23,691] [   TRAIN] - Epoch=3/3, Step=40/179 loss=0.0196 f1_score=0.8441 lr=0.000050 step/sec=4.48 | ETA 00:03:22\n",
      "[2022-01-15 16:58:25,925] [   TRAIN] - Epoch=3/3, Step=50/179 loss=0.0235 f1_score=0.7726 lr=0.000050 step/sec=4.48 | ETA 00:03:20\n",
      "[2022-01-15 16:58:28,159] [   TRAIN] - Epoch=3/3, Step=60/179 loss=0.0219 f1_score=0.8117 lr=0.000050 step/sec=4.48 | ETA 00:03:18\n",
      "[2022-01-15 16:58:30,388] [   TRAIN] - Epoch=3/3, Step=70/179 loss=0.0208 f1_score=0.8211 lr=0.000050 step/sec=4.49 | ETA 00:03:16\n",
      "[2022-01-15 16:58:32,628] [   TRAIN] - Epoch=3/3, Step=80/179 loss=0.0237 f1_score=0.8170 lr=0.000050 step/sec=4.46 | ETA 00:03:15\n",
      "[2022-01-15 16:58:34,850] [   TRAIN] - Epoch=3/3, Step=90/179 loss=0.0220 f1_score=0.8048 lr=0.000050 step/sec=4.50 | ETA 00:03:13\n",
      "[2022-01-15 16:58:37,090] [   TRAIN] - Epoch=3/3, Step=100/179 loss=0.0209 f1_score=0.8382 lr=0.000050 step/sec=4.46 | ETA 00:03:11\n",
      "[2022-01-15 16:58:39,345] [   TRAIN] - Epoch=3/3, Step=110/179 loss=0.0243 f1_score=0.7896 lr=0.000050 step/sec=4.43 | ETA 00:03:10\n",
      "[2022-01-15 16:58:41,594] [   TRAIN] - Epoch=3/3, Step=120/179 loss=0.0223 f1_score=0.8048 lr=0.000050 step/sec=4.45 | ETA 00:03:08\n",
      "[2022-01-15 16:58:43,842] [   TRAIN] - Epoch=3/3, Step=130/179 loss=0.0246 f1_score=0.8056 lr=0.000050 step/sec=4.45 | ETA 00:03:07\n",
      "[2022-01-15 16:58:46,085] [   TRAIN] - Epoch=3/3, Step=140/179 loss=0.0219 f1_score=0.7974 lr=0.000050 step/sec=4.46 | ETA 00:03:06\n",
      "[2022-01-15 16:58:48,356] [   TRAIN] - Epoch=3/3, Step=150/179 loss=0.0209 f1_score=0.8117 lr=0.000050 step/sec=4.40 | ETA 00:03:04\n",
      "[2022-01-15 16:58:50,606] [   TRAIN] - Epoch=3/3, Step=160/179 loss=0.0186 f1_score=0.8281 lr=0.000050 step/sec=4.44 | ETA 00:03:03\n",
      "[2022-01-15 16:58:52,857] [   TRAIN] - Epoch=3/3, Step=170/179 loss=0.0223 f1_score=0.8333 lr=0.000050 step/sec=4.44 | ETA 00:03:02\n",
      "[2022-01-15 16:58:55,680] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_f1_score=0.8038\n",
      "[2022-01-15 16:59:18,384] [    EVAL] - Saving best model to ./ernie_checkpoint_ner/best_model [best f1_score=0.8038]\n",
      "[2022-01-15 16:59:18,387] [    INFO] - Saving model checkpoint to ./ernie_checkpoint_ner/epoch_3\n",
      "[2022-01-15 16:59:31,826] [    EVAL] - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - Evaluation on validation dataset: \\ - Evaluation on validation dataset: | - Evaluation on validation dataset: / - Evaluation on validation dataset: - - [Evaluation result] avg_f1_score=0.8038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metrics': defaultdict(int, {'f1_score': 0.8038293033840759})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epoch = 3  # Number of epoches for fine-tuning.\n",
    "use_gpu = True  # help=\"Whether use GPU for fine-tuning, input should be True or False\n",
    "learning_rate = 5e-5  # Learning rate used to train with warmup.\n",
    "max_seq_len = 128  # Number of words of the longest seqence.\n",
    "batch_size = 32  # Total examples' number in batch for training.\n",
    "checkpoint_dir = './ernie_checkpoint_ner'  # help=\"Directory to model checkpoint\n",
    "save_interval = 1  # Save checkpoint every n epoch.\n",
    "\n",
    "\n",
    "# 选择模型、任务和类别数\n",
    "model = hub.Module(name='ernie', task='token-cls',\n",
    "                   label_map=MyDataset.label_map)\n",
    "\n",
    "train_dataset = MyDataset(tokenizer=model.get_tokenizer(),\n",
    "                          max_seq_len=max_seq_len, mode='train')\n",
    "dev_dataset = MyDataset(tokenizer=model.get_tokenizer(),\n",
    "                        max_seq_len=max_seq_len, mode='dev')\n",
    "test_dataset = MyDataset(tokenizer=model.get_tokenizer(),\n",
    "                         max_seq_len=max_seq_len, mode='test')\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(\n",
    "    learning_rate=learning_rate, parameters=model.parameters())\n",
    "trainer = hub.Trainer(\n",
    "    model, optimizer, checkpoint_dir=checkpoint_dir, use_gpu=use_gpu)\n",
    "trainer.train(train_dataset, epochs=num_epoch, batch_size=batch_size, eval_dataset=dev_dataset,\n",
    "              save_interval=save_interval)\n",
    "# 在测试集上评估当前训练模型\n",
    "trainer.evaluate(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码块如有运行问题，可直接运行funetune_ernie-ner.py源文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果预测\n",
    "如要对验证集的进行预测并对其指标进行计算，可对file_path进行修改，并自行增加指标计算。由于训练过程中的指标已是F1，与评测指标相同，实际无需再自行计算比较。\n",
    "\n",
    "我在本任务的训练中得到的最好的模型参数保存在`/ernie_checkpoint_ner_9505_2_f177.19/best_model/model.pdparams`以及`/ernie_checkpoint_ner_9505_3_f180.38/best_model/model.pdparams`（由于AI Studio平台限制，公开文件大小不超过1G，参数文件可能会在其他平台公开）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T08:59:55.931061Z",
     "iopub.status.busy": "2022-01-15T08:59:55.930489Z",
     "iopub.status.idle": "2022-01-15T09:01:02.808676Z",
     "shell.execute_reply": "2022-01-15T09:01:02.807669Z",
     "shell.execute_reply.started": "2022-01-15T08:59:55.931018Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-01-15 16:59:56,887] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "[2022-01-15 17:00:07,114] [    INFO] - Loaded parameters from /home/aistudio/ernie_checkpoint_ner/best_model/model.pdparams\n",
      "[2022-01-15 17:00:07,236] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/data122751/ner_finaltest_data.tsv\"\n",
    "text = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "label_list = ['B-BANK', 'I-BANK', 'B-PRODUCT', 'I-PRODUCT',\n",
    "              'B-COMMENTS_N', 'I-COMMENTS_N', 'B-COMMENTS_ADJ', 'I-COMMENTS_ADJ', 'O']\n",
    "label_map = {idx: label for idx, label in enumerate(label_list)}\n",
    "data = [[i] for i in text[1]]\n",
    "\n",
    "model = hub.Module(name='ernie', task='token-cls',\n",
    "                   load_checkpoint='./ernie_checkpoint_ner/best_model/model.pdparams', label_map=label_map)\n",
    "results = model.predict(data, max_seq_len=128, batch_size=1, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "部分结果展示，并保存到文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-15T09:01:16.229144Z",
     "iopub.status.busy": "2022-01-15T09:01:16.228495Z",
     "iopub.status.idle": "2022-01-15T09:01:16.263797Z",
     "shell.execute_reply": "2022-01-15T09:01:16.262967Z",
     "shell.execute_reply.started": "2022-01-15T09:01:16.229106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: [['不', '能', '?', '?', '好', '像', '房', '贷', '跟', '信', '用', '卡', '是', '分', '开', '审', '核', '的', '反', '正', '我', '的', '不', '得']] \n",
      " Label: O, O, O, O, O, O, B-PRODUCT, I-PRODUCT, O, B-PRODUCT, I-PRODUCT, I-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O\n",
      "\n",
      "Data: [['我', '感', '觉', '这', '样', '才', '是', '合', '理', '的', '，', '花', '呗', '白', '条', '没', '要', '那', '么', '多', '信', '息', '，', '照', '样', '可', '以', '给', '额', '度', '。', '有', '征', '信', '威', '慑', '，', '没', '那', '么', '多', '人', '敢', '借', '了', '不', '还', '。', '与', '其', '眉', '毛', '胡', '子', '一', '把', '抓', '，', '还', '不', '如', '按', '额', '度', '区', '间', '对', '客', '户', '进', '行', '不', '同', '程', '度', '的', '调', '查', '，', '免', '.', '.', '.']] \n",
      " Label: O, O, O, O, O, O, O, B-COMMENTS_ADJ, I-COMMENTS_ADJ, O, O, B-PRODUCT, I-PRODUCT, B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I-COMMENTS_N, O, O, B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I-COMMENTS_N, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O\n",
      "\n",
      "Data: [['这', '几', '天', '也', '接', '到', '了', '建', '行', '的', '分', '期', '电', '话', '，', '让', '我', '1', '.', '6', '分', '1', '2', '期', '，', '跟', '他', '说', '5', '0', '0', '0', '，', '3', '期', '，', '意', '思', '意', '思', '。', '快', '两', '年', '了', '还', '没', '首', '提', '，', '心', '塞']] \n",
      " Label: O, O, O, O, O, O, O, B-BANK, I-BANK, O, B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-COMMENTS_N, I-COMMENTS_N, O, B-COMMENTS_ADJ, I-COMMENTS_ADJ\n",
      "\n",
      "Data: [['我', '是', '建', '行', '贷', '款', '，', '别', '的', '银', '行', '信', '用', '卡', '没', '还', '，', '账', '单', '大', '概', '4', 'w']] \n",
      " Label: O, O, B-BANK, I-BANK, B-PRODUCT, I-PRODUCT, O, O, O, O, O, B-PRODUCT, I-PRODUCT, I-PRODUCT, O, O, O, B-COMMENTS_N, I-COMMENTS_N, O, O, O, O\n",
      "\n",
      "Data: [['银', '行', '耍', '无', '赖', '啊']] \n",
      " Label: O, O, O, B-COMMENTS_ADJ, I-COMMENTS_ADJ, O\n",
      "\n",
      "Data: [['晒', '晒', '“', '断', '”', '卡', '，', '我', '的', '招', '行', '卡', '猫', '咪', '卡']] \n",
      " Label: O, O, O, O, O, O, O, O, O, B-BANK, I-BANK, B-PRODUCT, O, O, I-PRODUCT\n",
      "\n",
      "Data: [['交', '行', '[UNK]', '1', '9', '7', 'k', '民', '生', '[UNK]', '2', '1', '7', 'k', '中', '行', '[UNK]', '1', '2', '5', 'k', '工', '行', '[UNK]', '1', '5', '8', 'k']] \n",
      " Label: B-BANK, I-BANK, O, O, O, O, O, B-BANK, I-BANK, O, O, O, O, O, B-BANK, I-BANK, O, O, O, O, O, B-BANK, I-BANK, O, O, O, O, O\n",
      "\n",
      "Data: [['工', '商', '8', '4', '0', '0', '0']] \n",
      " Label: B-BANK, I-BANK, O, O, O, O, O\n",
      "\n",
      "Data: [['2', '0', '1', '3', '0', '5', '招', '商', '银', '行', '2', '0', '1', '5', '0', '1', '建', '设', '银', '行']] \n",
      " Label: O, O, O, O, O, O, B-BANK, I-BANK, I-BANK, I-BANK, O, O, O, O, O, O, B-BANK, I-BANK, I-BANK, I-BANK\n",
      "\n",
      "Data: [['中', '信', '3', '4', 'k', '（', '淘', '宝', 'v', '、', 'i', '白', '、', 'l', 'u', '.', 'x', 'u', '.', 'r', 'y', '）']] \n",
      " Label: B-BANK, I-BANK, O, O, O, O, O, O, O, O, B-PRODUCT, I-PRODUCT, O, O, O, O, O, O, O, O, O, O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(data[0])\n",
    "# print(results[0])\n",
    "\n",
    "resultlist = []\n",
    "for idx, text in enumerate(data):\n",
    "    resultlist.append(\" \".join(results[idx][1:len(text[0])+1]))\n",
    "    if idx < 10:\n",
    "        labels = results[idx][1:len(text[0])+1]\n",
    "        print(\n",
    "            f'Data: {text} \\n Label: {\", \".join(results[idx][1:len(text[0])+1])}\\n')\n",
    "\n",
    "resultsDF = pd.DataFrame(data=resultlist)\n",
    "resultsDF.to_csv('ner_result.csv', sep='\\t', header=None,\n",
    "                 index=False, columns=None, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可能的提升改进方法\n",
    "\n",
    "因为时间仓促，参考的资料较少，也没有很多时间进行调参和优化，这一模型的很多参数都使用了默认值。如果需要进一步优化，一方面可以通过调整学习率等参数，另一方面可以[相关文档和教程](https://aistudio.baidu.com/aistudio/projectdetail/2463239)进一步选用ERNIE-Gram，或加入条件随机场（CRF）等方式来提升序列标注的效果。\n",
    "\n",
    "另外，从多次实验中可以发现，由于数据集分割是随机打乱后进行的，训练效果也受到数据集的制约。如果能寻找到一个好的数据集划分，也可以训练得到一个好的模型，但这是可遇而不可求的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
